<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<title><![CDATA[My Octopress Blog]]></title>
	<link href="http://yoursite.com/atom.xml" rel="self"/>
	<link href="http://yoursite.com/"/>
	<updated>2017-06-27T23:41:11-04:00</updated>
	<id>http://yoursite.com/</id>
	<author>
		<name><![CDATA[Your Name]]></name>
		
	</author>
	<generator uri="http://octopress.org/">Octopress</generator>
	
	<entry>
		
			<title type="html"><![CDATA[端到端QoS网络设计(第一章)]]></title>
		
		<link href="http://yoursite.com/blog/2017/06/27/title/"/>
		<updated>2017-06-27T23:18:09-04:00</updated>
		<id>http://yoursite.com/blog/2017/06/27/title</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>开一本专门讲QoS的书,原因其实很简单,在最开始学网络的时候,QoS这一块就是被稀里糊涂讲过去的.由于当时懂的太少,所以自己也是稀里糊涂听过去的. 几年过去了,每当提到这一块心里都觉得是稀里糊涂的. 很多人搞的很乱,我觉得最大原因是因为QoS比较抽象,动不动就成为一个单纯的数学问题. 这本书其实很长,我会有选择的读一下,对于一些看着就没太大用的玩意(比如IPV6的QoS)就暂且跳过.</p>

<h2>Introduction and History</h2>

<p>客观来说,这本书的作者是非常的啰嗦. 开始说了一大堆废话. QoS这个需求其实一直都存在, 随着网络的发展,这个需求更明显了.这件事的本质其实在于应用的发展远超过网络的网站,所以应用需求被区别对待.书中也列出了以下几点原因:</p>

<ol>
<li>流量矩阵变得更复杂. 由于应用的发展很迅猛,网络中的流量开始变的更复杂</li>
<li>视频发展的太快了. 在2016年底,互联网的流量百分之86来自于视频.</li>
<li>设备发展的快,技术更新没那么快(比如无线技术,ATM,frame relay). 这种情况下带宽就成了限制.</li>
<li>QoS自己也变得更加复杂. 这块是由于新的RFC标准和各种各样复杂的工具造成的</li>
<li>无线网络发展的很迅猛. 现在无线非常常见, 估计未来也只有server才需要有线了.</li>
<li>QoS也变成了一个安全的工具. 根据流量的特性或者根据QoS分析来把一些traffic drop来实现安全</li>
</ol>


<p>作者提出的这些论点中,其实我也就认1,2,5.其他觉得扯淡居多.</p>

<h2>历史</h2>

<p>通常开始讲东西的历史,一般就只能让我想起来是教科书了. 这本书还是废话的说了非常多. 我们大概回顾一下. 网络的最开始是从PSTN网络发展的来的,后来呢,就有了所谓的虚拟连接(PVC)的概念,这就是可能大家接触到的frame relay,这样网络整体就是基于数据包在交换,来提升网络的使用效率. 但是就是使用网络更加有效率这件事情,使得在网络上面的视频和音频这种实时流量不能被网络设备以实时的程度交付到客户. 这样就导致了流量其实是需要被区别对待的.</p>

<p>最开始是CoS 在APPN网络当中, 我也不知道APPN网络. 当时的CoS和现在的QoS还是很不一样的. 对于网络中的流量,其实是有三个最重要的特性来描述他是否拥塞</p>

<ol>
<li>delay(latency).其实就是端到到之间总共要花费多久的时间.</li>
<li>jitter. 其实就是两个包之间有多少延迟,开始我对这个理解有些问题,总觉得和delay差不多.jitter的问题更像是,开始一个网络卡了,你习惯了这种卡的程度,结果出现了卡的不匀速.</li>
<li>丢包. 丢包就是丢包了</li>
</ol>


<p>现在我们基本把流量分为三大种,下图就描述了语音,视频,数据这三种流量的特点.</p>

<p><img src="http://yoursite.com/images/data_type.png"></p>

<p>其实我对于为啥video是bursty我不是很理解的, 当然现在据说一些视频网站(比如toutube)也其实在用TCP.</p>

<p>无线由于有一层完全不一样的MAC层,所以QoS是完全另外一件事了.</p>

<h2>QoS基础概念</h2>

<p>此处作者又提出了几个没用的概念,QoE和QoX. 其实QoE就是用户实际的体验,QoS就是实现这种体验的手段而已.</p>

<p>其实QoS简单来说有两种手段: IntServ和DiffServ. IntServ理解起来很简单,就是每个节点都预留带宽.就好像有从A城市到B城市有一个车道是预留的,没有其他人可以用. 现在还在用这种QoS技术的,我只能想到MPLS TE的RSVP.</p>

<p>DiffServ 就是使用IP包里面的IPP,或者说DSCP来进行标记. 现在大多数QoS都是使用这种方式. 现在无线和视频对于的要求又有了一些变化,所以RSVP又开始被使用起来.</p>

<h3>QoS概念和工具</h3>

<p>QoS在实现的时候其实用了以下工具来达到目的.</p>

<ol>
<li>分类和marking. 一般就是通过包头特征来判断这个数据流将数据包进行分类,并且mark下来. 通常做marking是在进入一个网络的时候做的.</li>
<li>policing, shaping. 当流量被标记之后,我们会对流量进行资源分配.当流量超过他们所限定资源的时候,流量就会被被丢弃(policing),或者减速(shaping), 再或者重新标记.</li>
<li>拥塞控制或者排队机制. 当数据包在接口出现排队的时候,我们需要用到排队机制或者拥塞管理简直来帮助流量以更快的方式送出接口</li>
<li>link工具. 有些link需要特殊的工具,比如分片.</li>
</ol>


<p>这个图就基本阐述了QoS的工作过程.</p>

<p><img src="http://yoursite.com/images/qos.png"></p>

<h3>QoS的简化</h3>

<p>这块基本就是说Cisco的MQC工具.就是我们日常接触到的配置工具的policy map和class map等等.</p>

<h3>QoS的标准</h3>

<p>由于我们在用QoS的时候大多数时候就在对着IPV4的DSCP字段进行标记.其实RFC也指定了相关的标准来说那些类型的流量用什么样的DSCP字段. 实际过程中,路由器是不知道任何标准的,还只是根据你的配置执行.下图就是RFC的标准总结.</p>

<p><img src="http://yoursite.com/images/qos_rfc.png"></p>

<h2>Summary</h2>

<p>总体来说这一章废话比较多,回顾了历史.看了眼现在. 其中真正讲的内容不过是QoS的起因.两种QoS的手段(IntServ和DiffServ),以及QoS的基础工具和思路.</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第八章,QoS)]]></title>
		
		<link href="http://yoursite.com/blog/2017/03/24/inside-cisco-ios-software-architecture-8th/"/>
		<updated>2017-03-24T17:17:59-04:00</updated>
		<id>http://yoursite.com/blog/2017/03/24/inside-cisco-ios-software-architecture-8th</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章讲QOS,其实QOS我是有打算通过另外一本书来说的.这本书最后一章也只讲了出向排队的几种方式和一些命令,在此我们只讲书上的东西,超纲的东西我们通过其他书再讲.</p>

<h1>QoS简述</h1>

<p>书中提到QoS的背景,本质还是由于链路带宽不够用,路由器要区别对待不同类型的包.最常见的就是在拥塞的时候,路由器转发数据包选择先发什么后发什么,这叫做拥塞管理,或者是用一种方法避免拥塞.</p>

<ul>
<li>拥塞管理就是要再把数据包发送到出接口的时候,根据包的重要程度来选择先发什么,后发什么,从而避免对网络的影响</li>
<li>拥塞避免就是限制排队来降低发送速度</li>
</ul>


<h2>拥塞管理</h2>

<p>拥塞管理本质就是如何在一个出向接口排队,但是由于为了性能原因,有些排队策略的实现也和设备本身有关.这里我们主要讨论几种无关于硬件的排队方式. 其实这更像是一个单纯的数学问题,而非网络问题.这里涉及的排队策略有三种:</p>

<ul>
<li>Priority Queue(PQ)</li>
<li>Custom Queue(CQ)</li>
<li>Weighted Fair Queue(WFQ)</li>
</ul>


<p>我们之前讲过数据包是如何转发的.出向排队策略就是在transmit ring之前和排队到这个接口的queue之间.由于trasmit ring是FIFO(先到先出)的,过了trasmit ring的数据包就到了物理接口,最后被转发出去.所以数据包在转发的时候都是要放到接口的output queue,然后根据排队策略放到transmit ring的内存空间上面</p>

<h3>Priority Queue</h3>

<p>PQ策略很简单,priority高的永远优先.就好像是去银行排队,你如果是VIP客户,下一个永远叫的是VIP,VIP之间就是FIFO. 缺点当然也很明显,如果优先级比较高的很多的话,优先级比较低的可能会永远排不到. 就好像你去银行排队,你看着你前面有一个人,结果一会来一个VIP.你发现你可能一辈子都排不上.</p>

<p>我们用配置和例子来讲述下,相关的查看命令需要的话就查查config guide.</p>

<p><img src="http://yoursite.com/images/PQ.png"></p>

<p>这个图可以看到这个在S0接口调用了PQ这个策略,PQ中然后分为high, medium, normal.</p>

<p>数据包根据ACL匹配把数据包分到3个不同的queue当中. 当实际转发的过程中,就是会数据包2,4先发送,然后5,最后是1,3,6.只有当high空的时候才能发medium的数据包,当medium也为空的时候,才能发normal里面的数据包</p>

<h3>Custom Queue</h3>

<p>CQ其实是对于PQ的一种优化.避免了优先级比较低的数据包永远发不出去的窘境.其实就是在转发高优先级的时候转发到一定程度就不发了,然后去看次优先级的包.这样循环下去,从这个角度来看是个round robin.具体发到什么程度,通过一个例子就可以说的很清楚了.</p>

<p><img src="http://yoursite.com/images/CQ.png"></p>

<p>可以看到配置的时候设置不同的queue有不同的byte-count,这个参数其实有点像token一样.实际转发中,其实就是说你能转发多少bytes.</p>

<p>图中你也可以看到初始状态下面不同queue中有不同的包,首先,queue1可以转发1000bytes,这样数据包2可以被转发走.然后还可以转发500,下个数据包是4.虽然token不够了,依旧转发走.其实就是这个queue当中当byte大于0的时候一直可以转发,直到可转发的bytes小于0.queue2可以转发1000个,但是数据包5只有200bytes,这样转发完数据包5就去queue3转发数据包.到了queue3,数据包1,3,6都会被转发走.这样再回到queue1.
我们大体可以说queue1,2,3是按照1:1:2的比例转发的.当然这也要看具体数据包当时的情况.但是可以肯定的是说CQ是保障了高优先级的同时,也让低级优先级的也转发走.</p>

<h3>Weighted Fair Queuing</h3>

<p>这样其实又根据设备不同分了可以在线卡上面直接处理的和要在控制在CPU处理的.我们在这主要讲述和平台无关的 Flow-based WFQ和Class based WFQ.</p>

<p>我们很经常听到CBFWQ,很少再听到Flow-based WFQ.其实就是因为Flow based WFQ就是CBFWQ的更简陋版本.</p>

<h4>Flow-based WFQ</h4>

<p>这里其实就是根据数据包的某些字段(源目IP,TOS字节,TCP/UDP端口号)来定义一个流,然后对于这个流进行WFQ排队.既然是weighted,那自然要计算这个数据包的权重值.</p>

<p>权重值=4096/(IP precedence+1)</p>

<p>然后根据权重值算出来他们的序列号,最后根据序列号大小排队.</p>

<p>序列号=当前序列号+(权重*包长),当实际排队进入transmit queue的时候,就取最小的序列号.</p>

<p>此处比较绕的就是当前序列号,这个就要分情况看. 当一个数据包计算序列号的时候,先看自己流是不是空的,如果是空的.就用当前传输的序列号做当前序列号,如果不是空的.就用这个流队列的序列号进行计算.</p>

<p>此处我们用一个例子说明.假如我们有3个流,ABC,有5个数据包</p>

<ul>
<li>A1,A2-都是500bytes IPP=0</li>
<li>B1,B2-400butes IPP=0</li>
<li>C1-40 bytes IPP=3</li>
</ul>


<p>最开始A1先到,A1权重值是4095,序列号就是 4096*500+0(因为第一个包,当前序列号为0)=2048000,A1进入A队列</p>

<p>A2在A1之后到,A2的序列就是4096*500+204800=4096000,因为A2也要进入A队列,但是A队列不为空,A2使用的当前序列号就是A1的序列号.</p>

<p>B1,B2在A2之后到要进入B队列,假如这个时候A1要被发出去. B1计算序列号就不能是按0计算,而是按A1的序列号计算.B1=2048000+(4096+400)=3686400,B2类似于A2.B2=3686400+(4096*400)=5324800</p>

<p>这个时候假如A1发送出去了,那下一个要发送出去的就是B1而非A2.
这个时候C1数据包进来,同理由于C队列是空的,C数据包的序号就是3686400+(1024*40).</p>

<p>你可以发现这个时候C1数据包在A2,B2之前就发出去了.</p>

<p>我们这样做的根本目的是为了什么呢?很明显,我们是根据数据包的TOS和数据包的大小来排队把数据发出去. TOS字段很好理解,是因为这个区分了不同等级的服务.考虑数据包包长的原因是因为,一个接口传送数据的速度是有限的.在相同情况下,能传送更多小数据包,从而提高整个系统的效率.比如大家去银行排队,有些人就是去柜台弄个存款业务,很快就搞定了,如果让这些人插队.这样虽然不公平,但是对于银行来说整个效率是提高了,减少了所有人的平均等待时间.</p>

<p>在网络中,比如说你路由器同时处理一个telnet和一个FTP流量,telnet流量很小,但是对于延迟的容忍度就低一些.FTP一般数据量大的多,慢个一时半会也不会被发现.这样的情况下,WFQ就自动帮telnet加速,你基本完全不用配置.</p>

<p>其余的具体配置方法部分就不在这进入深入,有兴趣可以查看config guide</p>

<h4>Classed-Based WFQ</h4>

<p>这个就是我们现在非常容易用到的CBWFQ,在这个书上说CBFWQ还是一个非常先进的技术.</p>

<p>这个就是大家现在日常使用的MQC配置方法了,先配置一个class,在这个class里面定义你想匹配的流量类型,然后针对于这个class在policy-map当中进行处理,最后在接口调用这个policy-map就好了.</p>

<p>在这个里面我们非常常用的两个参数bandwidth和priority(其实就是LLQ)在这个地方需要单独拿出来说一下.</p>

<p>举个例子</p>

<p><img src="http://yoursite.com/images/CBWFQ.png"></p>

<p>图上说的priority和bandwith这两个命令就是CBWFQ我们常用的排队命令. Cisco官方也有一个表专门解释他们的区别,其实这也是一方面说明了很多时候这个玩意是疑惑到很多人的.</p>

<p><img src="http://yoursite.com/images/priority_bandwidth.png"></p>

<p><a href="http://www.cisco.com/c/en/us/support/docs/quality-of-service-qos/qos-packet-marking/10100-priorityvsbw.html">Cisco解释链接</a></p>

<p>我个人觉得以这样的理解方式会好很多.首先priority这个命令,其实就是在CBWFQ当中创建了一个CQ,这个CQ永远会被优先处理,你配置的priority xx,xx就是相当于给这个PQ多少的token,类似于刚才CQ提到的传输多少个bytes一样.当然如果这个CQ是空的话,token是可以被其他队列使用的. 举个例子,你去银行排队,一个小时银行可以处理20个人,如果我们配置了priority 5,意思是说我优先处理5个VIP客户,其他15个人正常处理.那如果一个小时先来了5个VIP,15个普通客户,那银行就先处理5个VIP.然后再处理15个人.当处理15个人的过程中,突然又来了5个VIP,这5个VIP就下个小时再处理了. 但是如果没有VIP客户来,或者来了20个VIP客户.这些其实对于银行来说都不算拥塞,在一个小时就处理完了.</p>

<p>对于bandwidth命令,我们是说这个class,我有多少的权重值来转发数据包.比如你是一个10M的口,这个地方你配置百分之20,或者2M,都是要把百分之20的权重给这个class,不同于刚才提到的CQ,这个百分之20并非会被优先处理,而仅仅是有百分之20的权重.比如说10M的口,2M的带宽.可能是每10个包处理的时候有2个包给了这个class,并非像是priority 命令一样.完全是先把这2M处理掉. 关于更细节CBWFQ的内容,这本书没有提及,我也不过多意淫.等到具体QoS书籍的时候,会有更详细的内容.</p>

<h4>Distributed Queue</h4>

<p>如果你考虑下之前的转发过程,会发现如果系统CPU介入QoS排队,还是非常消耗系统资源的.Distributed Queue本质就是说让线卡级别的CPU完成排队,数据包也不需要在SRAM和DRAM之间复制来去.具体内容书上也没有说的很清楚,我也不方便意淫太多.</p>

<h4>Modified Deficit round robin</h4>

<p>这个是12000系列路由器来管理排队的一种方式,其实看上去是个CQ的或者说是CBWFQ的变种.他其中有一个非常特殊的queue,这个特殊的queue有两个工作模式:</p>

<ul>
<li>strict priority queue,这个类似于PQ,不管怎么样,一定要先处理我的.后面的饿死也无所谓.</li>
<li>alternate mode,这个类似于CQ指定传输多少bytes.不同的就是他再处理其他queue之后一定会返回这个特殊queue</li>
</ul>


<p>举个例子,我语言能力实在有限.</p>

<ul>
<li>queue 0 能传输1500bytes,是特殊queue,在alternate mode下.</li>
<li>queue 1 能传输3000bytes</li>
<li>queue 2 能传输1500bytes</li>
</ul>


<p><img src="http://yoursite.com/images/12000_queue.png"></p>

<p>1/250表示这是第一个包,有250bytes大小. 首先数据包1,可以被传输,然后还能传输1250bytes,继续数据包2能被传输.这样queue0就是-250.然后去queue1处理数据包,接下来数据包4,5被传输出去,queue1能传输的数据变为0.这个时候,特殊就在于接下来并非要处理queue2,而是回到queue0,继续处理queue0中的数据.这个时候queue0能处理的数据是-250+150=1250.然后数据包3就传输出去了.这个时候queue就是空,可传输数据会被置为0.然后处理queue2,非queue1.这个时候数据包queue2可以传输1500bytes.这样数据包7,8,9,10都能被传输走.这个时候queue2能传送的数据包为0,回到queue1.然后就是传输数据包6,然后queue1空了,再传输queue2中的数据包11.</p>

<h2>拥塞避免</h2>

<h3>RED</h3>

<p>我们这里主要通过random early detection(RED)从而把TCP连接降速来避免拥塞. TCP传输速度是通过一个滑动窗口来控制的.滑动窗口会根据现在的速度不停的增大,直到出现拥塞.如果我们是一个FIFO的队列,结果就是导致了最后的包都被丢掉,由于丢掉的过多,然而会把窗口又搞的很小.</p>

<p><img src="http://yoursite.com/images/RED.png"></p>

<p>图中可以如果只是FIFO的情况下,TCP的带宽使用情况.为了避免出现这种情况,就出现了RED算法,就是发现不行了,早点开始丢包.这样让窗口在更早的时候缩小,而不会出现那么大的滑落.那RED算法中就有2个值我们需要考虑,min,max.</p>

<ul>
<li>当前速度低于min值,完全安全,不丢包</li>
<li>当前速度在min到max之间,开始丢,随着现在实际速度增长开始越丢越多.</li>
<li>高于max值,全都丢.</li>
</ul>


<h3>Selective packet discard</h3>

<p>当输入的包queue开始处理不过来了,这个时候IOS就开始选择性丢包,保证路由包或者是保持网络正常运行的包不会被丢弃.这个feature已经被IOS自己实现了,完全不需要你做任何配置.只要是被IOS识别出来重要的包.丢不会被丢弃的.</p>

<h2>其他</h2>

<p>这里其实没说很多QoS,比如说CAR,GTS,RSVP等等,因为毕竟这是一本讲IOS的书,并非专门讲QoS的书.我们会在其他的书中,把QoS这个单独拉出来说.</p>

<h1>summary</h1>

<p>这就是这本书最后一章,也是博客完成的第一本书的读书陪读笔记. 希望有人喜欢,觉得有用.</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第七章,12000系列路由器)]]></title>
		
		<link href="http://yoursite.com/blog/2017/03/22/inside-cisco-ios-software-architecture-7th/"/>
		<updated>2017-03-22T23:02:19-04:00</updated>
		<id>http://yoursite.com/blog/2017/03/22/inside-cisco-ios-software-architecture-7th</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章主要讲12000系列路由器,在此我们可以看到现代高端路由器的架构的样子.我们只是通过某个系列来了解这个类型的路由器的架构,所以关于平台的细节,我们可能就略过不再多说.</p>

<h1>硬件架构</h1>

<p><img src="http://yoursite.com/images/12000_hardware.png"></p>

<p>这里你可以看到几个最重要的部分</p>

<ul>
<li>switching fabric(交换矩阵)</li>
<li>Mbus(不太重要的总线)</li>
<li>route processor(主控卡)</li>
<li>line cards(线卡)</li>
</ul>


<h2>交换矩阵</h2>

<p>之前我们所有讲的架构其实都是整个系统有一个总线.他的缺点其实很明显,他总是要等到bus上面可以传输的时候,自己才传输.而且他还总是要传输到RSP的MEMD区域然后决定最后这个包要去哪.这两个问题是这种bus架构与生俱来的,再快的bus也无法解决这种问题.</p>

<p>12000架构路由器就利用的是crossbar的交换矩阵来作为背板,从而做转发.如图你可以看到转发成为一个矩阵,这样的情况下.数据包可以通过交换矩阵直接从一个线卡到另外一个线卡.</p>

<p><img src="http://yoursite.com/images/cross_bar.png"></p>

<p>讲一下交换矩阵, 上图就表示了一个交换矩阵的示意图,可以看到任何两个LC(线卡)之间都有交点,这便是线卡之间的通道.</p>

<p>关于交换矩阵,我们在这要提出几个基本概念: 交换矩阵, 交换平面,交换卡.</p>

<h3>交换矩阵, 交换平面</h3>

<p>交换矩阵和交换平面实际是都是逻辑概念. 一个交换矩阵是由多个交换平面组成的.</p>

<p><img src="http://yoursite.com/images/switch_fabric.png"></p>

<p>这个图是我从弯曲评论讲CRS设备的博客上面盗来的. 这个图你可以清楚的看到,每个线卡都是连接fabric plane. 这个fabric plane便是交换平面.可以注意到的是其实每个线卡都连接了所有的交换平面.</p>

<h3>交换卡</h3>

<p>之前提到的都是逻辑概念,这个才是实际中见到的交换卡.有时候一个交换卡里面有一个交换平面,一个交换卡里面也可以有多个交换平面.这个完全看硬件的实现情况.</p>

<h3>Cisco Cells</h3>

<p>很有意思的是当数据包从线卡进入多个交换平面的时候,数据包并不是选择某一个交换平面来传输数据包,是把一个数据包再分割成多个cell分散到多个交换平面,来负载多个交换平面,当数据包到达某个线卡的时候,再次把多个cell组装成一个数据包. 具体这么做的原因我也不是很明白,可能涉及到更深层次的交换矩阵的实现, 因为如果负载交换平面也可以直接让数据包在多个交换平面之前做round robin来实现.</p>

<h2>maintenance bus</h2>

<p>这个就是用来维护系统的,比如启动的时候收集各个line card的信息,实际的数据信息是永远不会走这个bus的.</p>

<h2>CPU</h2>

<p>和之前说的类似,主要就是做实际的计算,然后把计算结果发送到各个线卡上面.具体的CPU, DRAM,SRAM和之前的路由器没有什么分别.</p>

<h2>Line card</h2>

<p>之前提到过,到了这个级别的路由器,实际设备已经是通过线卡自己就完成了转发.所以线卡大概有这三个部分构成</p>

<ol>
<li>physical layer interface module: 用来终结实际的物理口(可能是以太,ATM,POS)</li>
<li>Switch fabric interface: 和交换矩阵连接的部分</li>
<li>Packet forwarding engine: 转发引擎.</li>
</ol>


<p>书中这里提到了具体12000系列不同代的引擎. 我们在此就不在多叙述.大概讲下其中不同的部分.简单来说线卡在设计上面是基本就是复制之前主控板的设计就基本OK的.CPU,DRAM和SRAM.只是后来线卡的实现的时候引入了ASIC芯片.这样在数据实际数据包的时候就利用了ASIC芯片,而非线卡上面的CPU.这样提高了转发效率. 在此你可以看到CPU的设计又再次分离.</p>

<h2>数据包转发</h2>

<p>在12000系列中,大多数数据包的转发都是通过线卡就转发走了.只是控制信令的数据包会被送到主控卡做数据交互.</p>

<p>由于线卡架构不同,有ASIC和没有ASIC的转发也会有所差距,这里我们尽量统一.</p>

<ol>
<li>物理硬件检测到有数据包到了接口,会把数据包先放进一个FIFO的内存空间中(这是之前系统没有的),在12000中叫burst memory.</li>
<li><p>从receiving ring里面找到一块内存空间,把数据包从burst memory复制到另外一个内存空间,然后等待CPU处理 (这个地方12000有他自己的名字,比如管理receiving ring和transit ring的叫buffer management ASIC).</p></li>
<li><p>给CPU发中断请求,查询CEF表.</p>

<ul>
<li>如果CEF表里面有,就在线卡上面直接改写,然后准备转发出去</li>
<li>如果不在CEF表中,直接丢弃.</li>
<li>控制信令包,转发到主控卡</li>
</ul>
</li>
<li>把数据包切成64bytes的cell,发送到交换矩阵里面.</li>
</ol>


<p>如果有ASIC 芯片 不同之处如下:</p>

<ol>
<li>没有区别</li>
<li>如果这个线卡是有ASIC芯片的,这个时候就直接读数据包的包头(前64bytes),就直接从burst memory复制到了ASIC CPU处理的内存区域.</li>
<li>ASIC芯片通过查找ASIC内存区域的CEF表来判断是否可以转发包.如果可以转发,就把数据包改写,然后通过receiving ring管理机制把数据包,把数据包切成64bytes,发送到交换矩阵.(这个过程没有给线卡CPU发送中断请求)</li>
<li>如果ASIC不能处理,就会走按之前说的方式中断CPU来转发数据包</li>
<li>trasmit ring把找到一块内存把cell重新组装成数据包.</li>
<li>最后就是数据包先从trasmit ring的内存空间到最后的物理接口.</li>
</ol>


<p>转发的细节流程图书上有更详细的解释.</p>

<h1>Summary</h1>

<p>这一章主要通过12000系列讲一下高级的路由器内部架构.可以发现看到现在高级路由器的架构,线卡,线卡直接转发,交换矩阵.单独的ASIC芯片处理数据包.看上去20年前的架构和现在的没有很大差别了.</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第六章,7500硬件架构)]]></title>
		
		<link href="http://yoursite.com/blog/2017/03/11/inside-cisco-ios-software-architecture-6th/"/>
		<updated>2017-03-11T18:48:38-05:00</updated>
		<id>http://yoursite.com/blog/2017/03/11/inside-cisco-ios-software-architecture-6th</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章主要讲7500系列路由器,在这个书出版的时候7500应该还是一个Cisco很高级的路由器,是之前提到的AGS和7000系列的升级版.</p>

<h1>7500路由器硬件架构</h1>

<p>这个图可以看到7500系列的架构,可以注意的是他用了Cbus的架构,这里最大的区别也只是bus上面速度更快了.
<img src="http://yoursite.com/images/7500_architecture.png"></p>

<h2>Route Switch Processor</h2>

<p>RSP就是之前所谓RP和SP一体转发的CPU.其实我也不是很能理解为啥会把SP和RP架构再次弄成一个RSP.可能在7500推出的时候CPU计算能力增强了,分布式的RP和SP之间还是有bus需要来交换数据,使用效率不是很高.不过这些都是猜测.</p>

<h2>Route switch processor</h2>

<p><img src="http://yoursite.com/images/route_switch_processor.png"></p>

<p>图中可以看到这个处理器的架构.CPU在此做了路由和转发的决定.</p>

<h3>MEMD</h3>

<p>这个便是用来存放packet buffer的内存空间.根据数据包的MTU大小来创造packet的buffer.具体算法这块过于细节,大概说起来就是这块内存区域会根据接口的MTU划分出来sub pool,每个sub pool会存放一个MTU区间,比如说pool A里面的packet buffer是存放MTU在256和512之间的数据包.</p>

<h3>Main Memory</h3>

<p>这个里面就是操作系统需要用到的内存,跑IOS的内存和各种进程需要用的内存.图中IDB就是接口上面的相关信息,这个是很早第一章中的内容,不过我也已经完全忘了.</p>

<h3>RSP 转发</h3>

<ol>
<li>接口收到数据包,接口上面有一个小处理器,会把数据包放入接口的内存当中</li>
<li>接口处理器尝试把数据包放到MEMD区域. 尝试先放入一个local MEMD中,如果local MEMD满了,就会放入global MEMD当中.(这点和之前说的public pool以及private pool应该是一样). 如果global MEMD也满了,这样就会把这个包丢了.</li>
<li>找到可以用的MEMD空间之后,把数据复制到MEMD里面</li>
<li>MEMD把数据包放入RSP的queue等待处理</li>
<li>此时会给CPU提一个中断.</li>
<li>CPU开始根据设备的配置处理这个中断(走CEF或者是fast switching).我们这里讲的是每个包都会出发中断.现实当中,一个中断会处理buffer中的很多包</li>
<li>CPU根据情况来处理这个包,CEF可以查到下一跳,就进入转发阶段.如果没有匹配到就会进入fast switching. fast switching如果还是没有匹配到的话,会触发process switching</li>
<li>process开始处理包,这个时候,CPU需要把包从MEMD复制到system内存当中.如果由于system里面没有内存空间了,或者排队要process switching的包太多了, 这个包会被丢弃.当CPU可以正常处理这个包,并且可以在output queue中找到空间的时候,IOS就会把sytem memory的包复制回MEMD.</li>
<li>IOS尝试会把MEMD的数据包放入接口的queue当中,如果queue满了,就放入系统的queue排队等待.</li>
<li>接口的处理器会把MEMD上面的包复制到自己的内存上面.</li>
<li>把数据包转发出去,然后清空自己的内存.(这块转发流程书中有详细的流程图)</li>
</ol>


<h2>VIP架构</h2>

<h3>CPU</h3>

<p>VIP(Verstiable Interface Processor)架构其实就是现在常见高端路由器上面的分布式架构,每个转发的板卡都有自己的转发系统.其实每个板卡上面的都独立运行着一个特殊版本的IOS,他会处理RSP同步过来的一些计算结果,也会直接改写下一跳的MAC地址,直接转发出去.</p>

<h3>内存</h3>

<p>这里也分给操作系统用的DRAM和处理包用的SRAM,只是不叫MEMD而已.</p>

<h3>VIP架构下的转发</h3>

<p>之前我们提到了分布式转发,其实就是线卡上面有一套同步于RSP的系统进行的转发, 我们再次看下在分布式系统下数据包的转发(流程图书中可以看到)</p>

<ol>
<li>接口检测到有数据包,会把数据包放到一个内存空间,这个就是接口的receive ring.</li>
<li>接口会直接给VIP的CPU发一个中断,然后数据包会尝试的放入到板卡上面的SRAM中.</li>
<li>如果可以用CEF,就直接通过CEF把数据包转发走. 如果不能通过板卡上面的CPU转发.将会通过cbus来把数据包转发到RSP上面.</li>
<li>如果出接口是在这个板卡下面,在这个VIP架构下,数据包就会直接转发走,不会通过cbus.这样情况下,数据包就直接进入转发ring,然后转发出去.</li>
<li>如果数据包出接口不在这个VIP架构之下.他就会尝试通过cbus把数据包转到RSP中的MEMD中,然后通过MEMD转到另外一个VIP架构下的出接口,但是这个过程RSP的CPU完全不用处理.</li>
<li>最后把数据包转发出去,清空当时数据包占用的内存空间.</li>
</ol>


<h3>VIP架构下面的接收buffer</h3>

<p>VIP架构下面,其实每个板卡上面的CPU和内存都相互独立,板卡的CPU是不可以访问其他VIP架构下面的内存空间.他们可以访问的只有自己的内存空间和RSP的MEMD.这个时候就会出现当他们想把数据包放入MEMD的时候,出现MEMD没有内存空间的情况. 如果MEMD没有空间,VIP就会暂时放入自己的内存空间.以保证数据包不会被丢弃.</p>

<h1>Summary</h1>

<p>主要就讲了下7500架构的系统,从前面几章到现在,其实可以看到硬件的架构和软件的架构是逐渐复杂的. 现在已经有了VIP架构和分布式转发,下一章我们会看到交换矩阵.</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第五章,Particle-Based Systems)]]></title>
		
		<link href="http://yoursite.com/blog/2017/03/06/inside-cisco-ios-software-architecture-5th/"/>
		<updated>2017-03-06T22:49:22-05:00</updated>
		<id>http://yoursite.com/blog/2017/03/06/inside-cisco-ios-software-architecture-5th</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章主要讲particle based的系统和落后的7200系列路由器,具体设备相关的内容就大概说一下,毕竟过时很久了.</p>

<p>particle based系统的主要优势就是更高效内存资源. 之前章节提到过,系统中的内存会划分成不同的区域给不同的MTU的数据包.这段内存区域是连续的,单个数据包占用的内存空间等于MTU的大小. 这样就导致系统资源的浪费. 比如接口MTU是4KB,然后很多数据包是1KB的.这样百分之75的内存空间都浪费了. Particle 就是解决系统浪费的问题的</p>

<h1>Particles Buffer Management</h1>

<p>particles buffer 是把多个不连续的内存区域考虑成一个逻辑区域.这样下来,一个数据包可能会打散到多个不连续的内存空间.这样的话的确提高了内存效率.只是系统内部的复杂度增高了.通常来说每个particle是固定的.下面就用一个例子讲下如何工作</p>

<p><img src="http://yoursite.com/images/particle_pool.png"></p>

<p>我们假设每个particle size是512bytes,P1,P2,P3. 我们收到了一个1200 bytes的数据包.</p>

<ol>
<li>发现P1是个空闲的,把前512Bytes复制到P1.</li>
<li>发现P2是空闲的.然后把让P1,P2连起来,然后把中间的512bytes复制进去</li>
<li>找到P3,连接P2,P3把最后的176 bytes复制进去.</li>
</ol>


<p>知道链表的人,已经完全看明白.其实这就是数据结构里面的链表的简单形式. 看这个图会更明显一些.</p>

<p><img src="http://yoursite.com/images/Particle_buffer_header.png"></p>

<p>其中数据包头自然就是数据包的头,particle header,其实指向下个particle的空间.我估计就是个指针.data block就是数据包实实在在的pay load. 大多数IOS系统就是用这种方式来管理buffer的.你可以用show buffer命令看到.每个接口像之前讲过的一样,分public和private的.</p>

<h1>Particle coalescing</h1>

<p>由于process switching的特殊性(具体原因我也不知道,不过我猜大概是CPU在内存上面搜索多个表再操作内存很耗费资源). 所以cisco决定当process switching的时候,就会把已经分开的包再组合到一起. 这个过程就叫coalescing.具体的操作过程也是挺废话,挺没意思的.这里就不多说了.大概就是找到一块够大的内存,把分散的数据包复制进去,然后清空原来内存.</p>

<h1>7200系列路由器</h1>

<p>这个老掉牙的路由器,书上还讲了不同的型号和不同的板卡,在这就不浪费时间了.直接大概讲下硬件结构.</p>

<p><img src="http://yoursite.com/images/7200_architecture.png"></p>

<p>这个用的是7200作为例子. 后面为了提高转发性能又搞了TDM的架构作为改进.这个地方不再多说那种架构,在我看来TDM架构就是为了兼容现有架构情况下的一种妥协而已.在后面章节你会看到有SP转发引擎的路由器架构.</p>

<h2>数据转发</h2>

<p>在7200上面，由于没有单独的转发引擎。所有的转发其实都是CPU在承担的。</p>

<h3>数据转发具体过程</h3>

<ol>
<li>数据先从实际的接口收到之后，复制到receive ring当中，这个receive ring有可能在IO内存或者在PCI内存当中。唯一的区别这里是数据包在进入内存的时候已经被打散成为particles。</li>
<li>发给CPU一个终端请求。</li>
<li>收到终端请求，根据接口private pool和系统public pool的情况放在pool当中。如果pool已经满了，就丢弃这个包。和之前讲的一样，先尝试放进private pool再试public pool</li>
<li>由于数据包放入了pool当中。这样receive的ring可以重新被填充。可以继续接受新的数据包</li>
<li>看CEF表或者cache表看是否可以直接转发走。如果不可以，重新把包组装起来给CPU。走进程转发，具体的进程转发就不在叙述一遍了</li>
<li>查找出接口，改下MAC地址.</li>
<li>放在output queue或者transmit ring当中（具体判断之前一章说过，不在此赘述）</li>
<li>具体接口把数据从内存copy到接口，告诉CPU。</li>
<li>CPU清理内存空间并让接口转发数据包</li>
</ol>


<h1>summary</h1>

<p>其实这一章主要讲了下高效使用内存的方式，其他7200路由器架构和转发，只是复习了原来的内容。虽然在具体操作的时候的确要考虑他是一个连续的数据包还是分散的。但是逻辑上面他是一个数据包，其实没有太大差别。</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第四章,早期Cbus路由器架构)]]></title>
		
		<link href="http://yoursite.com/blog/2016/11/23/inside-cisco-ios-software-architecture-4th/"/>
		<updated>2016-11-23T21:51:19-05:00</updated>
		<id>http://yoursite.com/blog/2016/11/23/inside-cisco-ios-software-architecture-4th</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章主要讲Cbus架构的路由器,其中书中提到了AGS,AGS+还有7000系列.其实已经退休了20年了,不过我们可以从这种路由器看到一些现在路由器的架构的雏形.
其中也会讲到数据包的转发过程.过于细节的东西就不会再提到了,因为实在没有太大意义.</p>

<h1>AGS+ 路由器物理架构</h1>

<p>AGS+和7000系列都是这个架构下的.与之前的shared memory的路由器不同的在于他们用了Cbus,其实就是总线.在Cisco用总线的时候,其实个人电脑还是没有总线的.</p>

<p>在这个Cbus总线架构下,设计的目的就是包的转发尽量少让CPU参与转发. 所以Cubs架构下,有了一个自己的CPU和自己的内存区域.这一块统称为Cbus controller.由于这个CPU是单独用于转发的,在指令集上面可以做很多针对于转发的优化,内存也可以选取性能更高的来优化转发速度.</p>

<p>下图就是Cbus架构.你可能会注意到还有个multibus.这个应该是主板不同结构通用的bus.这里还显示出来应该是因为AGS+当时为了向前兼容一些老的线卡所致.在后面讲的cisco 7000路由器系列中,转发就只有cbus了.而在AGS+这种路由器情况下,如果出接口是multibus上面的线卡.数据包就会在走一遍multibus.</p>

<p><img src="http://yoursite.com/images/Cbus_architecture.png"></p>

<h1>Cbus下数据包的转发</h1>

<p>我们之前讲到过转发,不论是那种转发方式,都是在把数据包放在内存之后,都会给CPU一个中断,CPU来做包转发,区别只是CPU要根据CEF,cache,或者自己根据路由表做转发.但是CPU其实还是有很多时候要处理别的事情的.</p>

<p>在Cbus架构中,只要数据包能在Cbus接口收到就好.这样Cbus的CPU就能自住处理.这样的转发方式叫做autonomous switching</p>

<h2>autonomous switching</h2>

<p>autonomous switching其实转发和快速转发差不多.基本上唯一的区别就是不同处理器而已.甚至在10.2IOS之前,autonomous switching也用hash表来实现的.</p>

<p>比较大的区别就是autonomous switching就是没有支持那么多协议,比如只支持IP IPX.</p>

<p>还有就是如果autonomous switching如果没有cache的话,他就会让CPU来处理,来尝试做fast switching甚至process switching.</p>

<h2>Cbus的转发的内存</h2>

<p>由于Cbus的接口只能到Cbus的总线,所以Cbus接口是不可能直接轻松访问到CPU的内存部分的.所以Cbus架构下有自己的内存区域.简称MEMD,其实就是SRAM.他和系统内存一样,根据不同的MTU来划分出来不同的buffer.</p>

<p>在Cbus的内存中也区分receive queue和transmit queue. 和CPU内存中的queue类似, 数据包也是从网卡复制到Cbus的内存,如果数据包可以被Cbus的CPU处理, 就直接在Cbus上面分到transmit queue 再到具体的接口. 如果需要fast switching和process switching,就会被转发到multibus的CPU上面来做处理. Cbus这个CPU也会决定接口的速度和记录一些转发的基础数据.其实在这里你可能会意识到,从Cbus这类路由器,转发和控制开始分离,这个所谓的cbus就是转发引擎的雏形.</p>

<h1>Cisco 7000</h1>

<p>Cisco 7000是代替AGS,AGS+系列路由器的,还干掉了所有用multibus的接口.换句话说所有接口都是连接到Cbus上面.</p>

<p>原来的控制CPU就是大家现在熟悉的RP,就是路由引擎.没有了原来又转发又传送控制信令的multibus, 在路由引擎和转发引擎有一个另外的bus.</p>

<p>Cbus controller也正式改名叫SP,也就是交换引擎.其他没太多实际变化.</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第三章,shared memory路由器)]]></title>
		
		<link href="http://yoursite.com/blog/2016/11/16/inside-cisco-ios-software-architecture-3rd/"/>
		<updated>2016-11-16T21:43:59-05:00</updated>
		<id>http://yoursite.com/blog/2016/11/16/inside-cisco-ios-software-architecture-3rd</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章，我们主要讲三个主题:</p>

<ul>
<li>shared memory 路由器的硬件架构</li>
<li>shared memory路由器的packet buffer</li>
<li>shared memory路由器 如何转发数据包</li>
</ul>


<p>shared memory路由器是最基本的一种路由器. 他们有两个很基本的特征</p>

<ol>
<li>他们的硬件资源基本裸露的.就直接是CPU,内存还有网络接口</li>
<li>他们都只用系统的buffer做数据包的buffer</li>
</ol>


<p>因为shared memory的路由器是最基本的路由器,硬件上面架构非常简单. 硬件架构如图:</p>

<p><img src="http://yoursite.com/images/memory_router.png"></p>

<p>可以看出来其实整个的体系架构和之前提到的是差不多的.简单来说就是从接口传到系统的内存,然后做处理.</p>

<h1>CPU</h1>

<p>CPU在这一块就是完全做一切处理,从运行IOS到转发数据包. show version可以看到具体CPU信息.</p>

<h1>内存</h1>

<p>shared memory就用DRAM作为内存,所有的路由表,运行IOS和packet buffer都在这个DRAM中.</p>

<p>IOS系统把DRAM分为两个逻辑区域,local和iomem. 大体来说local就是当作操作系统正常运行的内存空间,就是跑操作系统之类的.iomem就是所谓的shared memory区域,其实也是为啥这一类路由器叫shared memory.因为网卡和CPU都可以访问这块内存资源.</p>

<p>其实有很多系统,DRAM都是独立的,只跑操作系统.IO的memory也应该是物理独立的,并非像我们现在讲的路由器一样IO的memory也是从DRAM中分的.</p>

<p>管理内存的pool也有2个,一个是管理local的(其实就是操作系统运行过程中的内存分配和释放),另外一个管理IO memory部分的内存.</p>

<p>其实所有的路由器都是共享内存的.这种路由器被叫共享内存的最大原因是因为他们只有一个IO部分的共享内存,所有接口和处理器都访问.还是这块内存空间,还要做包转发.不像我们第一章之前提到的其他路由器,会因为访问问题,数据包会在内存之间相互转移,复制.</p>

<h1>接口控制器</h1>

<p>其实就是用来控制接口的.他们有自己的处理器,但是处理器不做任何的包转发行为.做的就是把网卡的包从网卡到之前提到的IO内存部分.</p>

<h1>packet buffer</h1>

<p>我们之前很早就提到过system的buffer就是一个用来转发数据包的.shared memory的路由器特别之处就在于所有的包转发都在这也就是所谓的public pool.其实这也是我们之前提到的.这块内存就是被CPU和网卡直接访问的.</p>

<p>由于所有人都可以访问到这块内存区域,可能会出现一个接口的数据把这独占了,导致其他接口数据无法转发的情况.所以每个接口其实还实现了private pool.这样他们在收到数据包的时候可以放在自己的pool当中.</p>

<p>如果你还有印象,你会记得之前讲到packet buffer是动态的用来满足路由器变化的需求.private pool由于每个接口都有,只是为了避免public buffer满了,导致数据无法转发的情况.private的pool是固定的,接口会先使用自己的private pool,如果不行,再去考虑public pool.</p>

<h1>接受和转发Ring</h1>

<p>在IO内存当中还有一个数据结构Ring,他用来表示能接受和转发数据包的数量.其中接受的ring主要取决于是什么样速度的接口,转发ring看接口是什么样的排队策略和接口速度.</p>

<p>接受的ring的大小一般是固定的.而转发ring是变化的,这事由于路由器自己会产生包.通过show controller可以具体看到ring在IO memory的情况.详细图片解释就不在这里详细讲了.</p>

<h1>数据包的转发</h1>

<p><img src="http://yoursite.com/images/share_memory_forwarding.png"></p>

<p>这个图展示了如何收数据包的,我凑合的用这个图解释下整个过程.</p>

<ol>
<li>先把数据包从接口复制到IO内存的ring当中.</li>
<li>告诉CPU有个包.这点很有意思的就是他告诉CPU有中断,就继续接收新的数据包了,不等待CPU.</li>
<li>这个时候CPU开始处理中断,然后把ring中的数据包尝试放到private pool,如果不成功再放到public pool,如果public pool也满了.这个包就会丢掉.当然这个地方只是private pool或者public pool的指针指向ring的地址而已,并没有出现数据真实的复制.</li>
<li>其实这个时候CPU就开始根据CEF信息或者cached的信息查转发的下一跳和出接口了.如果CEF中没有下一跳的MAC地址的话(就是CEF表里面的punt),其实他会尝试一下cache,如果cache再没有就会走回进程处理,把数据包放到input_queue上面.</li>
<li>如果真的要进程处理,进程就会尝试查找下一跳的二层信息,比如通过ARP.</li>
<li>转发出去的时候需要注意的就是这个output queue.不论是怎么转发的都要在output queue这个地方排队.当然如果output queue是空的话,就直接到了转发ring那边.(如果transmit ring那边满了,会开始在output queue排队,output queue也满的话,就直接丢弃了) 这个的原因很有意思,因为你第一个包可能需要查询,在CPU那边排队,发ARP,改写.但是你接受ring那个地方还是一直在收包.很有可能你刚更新CEF信息,CPU还在改写第一个包.如果这个地方不检查queue是不是空的,就可能会出现第二个包比第一个更早的到达转发的ring,导致包的乱序.</li>
<li>正常情况下,系统会尝试找到一个可以用的转发ring,然后让转发ring指向这个包.不过一定记得这个地方其实只有指针的变化,没有数据真实的复制.转发ring,output queue都只是指针.</li>
<li>接口周期查询转发ring,然后从内存复制数据到接口.给CPU一个中断转发中断.</li>
<li>CPU干活,清理原数据包内存空间.把output queue中的数据包继续送往转发ring.</li>
</ol>

]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第二章,基本转发模式)]]></title>
		
		<link href="http://yoursite.com/blog/2016/10/30/inside-cisco-ios-software-architecture-2nd/"/>
		<updated>2016-10-30T00:18:39-04:00</updated>
		<id>http://yoursite.com/blog/2016/10/30/inside-cisco-ios-software-architecture-2nd</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>这一章主要讲了这几个主要的转发模式:</p>

<ul>
<li>Process switching</li>
<li>Fast switching</li>
<li>Optimum switching</li>
<li>CEF</li>
</ul>


<p>一个路由器最主要的任务其实就是把一个包从一个网络转发到另外一个网络当中.
简单来说分为四部.</p>

<ol>
<li>包进入接口.</li>
<li>看包的源地址,比较路由表.</li>
<li>如果有match.转发.</li>
<li>没有match丢弃.</li>
</ol>


<p>说起来这个东西并不难,但是所有的难点不过是如何在有限的资源下面提高性能.其实IOS有很多种方式来尝试实现快速转发.</p>

<h1>Process Switching</h1>

<p>这个其实是最基础的转发,很耗费CPU资源,唯一的优势就是可以实现包级别的负载均衡.
这个图片说明一切.</p>

<p><img src="http://yoursite.com/images/process_switching.png"></p>

<ol>
<li>数据进入接口之后,会把数据包传入IO的内存部分(此处的IO memory就是之前上面提到的比较块的SRAM内存.但是上一章上面的内存图中IO memory显示为DRAM,而书中内容提到大多数情况IO memory是使用SRAM.前后矛盾,不过没必要研究这么仔细.大体来看就是数据包进入了)</li>
<li>接口会发一个中断请求给CPU,告诉那里有包处理.CPU会检查包信息,然后应该会把包放在合适的queue当中.这里就应该指的是我们之前提到的packet buffer的queue.</li>
<li>这个时候会通知ip_input进程去跑queue中的包,去做包处理</li>
<li>这个时候才到了我们一般理解的包处理的过程,查看目的地址是否跟路由表有match,如果有,尝试得到下一跳的IP信息.再从ARP表中得到这个IP地址和MAC地址的对应.修改目的MAC地址.</li>
<li>把数据包放到IO memory的outbound queue.</li>
<li>把数据包从IO memory放传送到实际接口中.剩下就是接口的事情了.</li>
<li>剩下就是接口通知CPU包已经到了接口,释放内存中空间.</li>
</ol>


<h2>Process switching的负载均衡</h2>

<p>其实这样的转发方式最大的优势就是能实现包级别的负载均衡.因为CPU很容易在多条路由条目之间做轮询.有些路由协议(EIGRP)也可以根据权重做非等价负载均衡.</p>

<p><img src="http://yoursite.com/images/process_traffic_share.png"></p>

<p>稍微解释下这个图里面的意思.星标就是下个包会用的路由条目,traffic share count指的是不同条目之间的比例.比如这个图中,流量会1:2的比例转发到10.1.2.1和10.1.3.1上面.这样的确做的确能把流量完全分开,有一个可能的缺点就是这样的负载均衡会把到同一个目的地址的数据包发到不同的链路,导致最后数据包到目的地址的顺序会乱.</p>

<h2>Process switching的缺点</h2>

<p>其实process switching最大的缺点就是查表的太多.而且路由表由于最长匹配原则,是一定要查完的,而且还要递归查询.当网络大起来的时候,路由表也很长,这样使用CPU资源会影响到包的转发速度.</p>

<p>另外一点就是影响内存数据传送时间.在某些平台上面,CPU不能直接访问IO内存区域.数据包要从IO内存刀其他内存空间,再转发出去的时候还要再把数据包复制回IO内存空间.想想这些就知道多浪费CPU资源.</p>

<p>在我们讨论具体的更好的方法之前,我们先看下ip_input这个进程到底做了什么.</p>

<ol>
<li>目的地址是否可达.其实就是查询路由表</li>
<li>出接口是谁.</li>
<li>封装二层信息.大多时候是MAC地址.通过封装下一跳的MAC地址,一般查询ARP表.</li>
</ol>


<p>因为路由表和ARP表都很大.查询起来很耗费资源.如果能有一个可达表加上出接口和目的mac地址的表就能帮忙提高转发速度.这样在包收到在最开始CPU的收包的中断的时候简单查一个表就解决了,也减少了数据包在内存上面的copy.在iO内存也就完成了.</p>

<h1>快速转发(cache)</h1>

<p>cache其实就是计算机行业里面很常用的,把经常访问的东西先存起来的一种方法.在IOS里面其实缓存了可达地址,出接口和MAC地址.</p>

<p><img src="http://yoursite.com/images/fast_switch.png"></p>

<p>看这个图,你会发现CPU介入之后直接查询cache,得到结果然后转发.ip_input进程根本不在涉及当中.其他步骤差不多,不过这里注意,此处CPU只有最开始的一个中断就完成了所有的需要的操作.这也就是一次查询多次转发的概念. 而且这个cache当网络不是变化很大的时候比较有用.如果互联网骨干之类的地方.很多cache会不匹配,cache过期等等问题,反而更麻烦. 从show ip route cache verbose可以看到具体的cache信息.另外cache在实现的时候用了其他的数据结构来提高效率.</p>

<h2>hash表和radix tree</h2>

<p>开始快速转发的表是用hash表实现的(如果不知道hash表的人需要复习下数据结构了). 简单的说就是对着目的地址做XOR操作作为hash表的index, 其中存下一跳的MAC地址. 对于hash表的冲突问题, 就把多个条目存在一个index下面, 如果有冲突的情况, 再做一次查询操作就好. 由于是IP地址,所以输入是基本可以预知的,再做一次查询增加的时间复杂度也可以接受.</p>

<p>在IOS 10.2之后(你没有看错,是10.2)实现方式从hash表变成了 radix tree. 具体radix tree,我也大概只知道是一个二叉树.单从查询上来看的话,时间复杂度应该hash表更有优势.但是具体radix tree总体的优势,我也没有再深入研究,经过一些搜索大概知道现在路由表一般都是用radix tree实现的. radix tree存储IP地址的方式就是把IP地址用二进制的方式然后把一样的byte组成树的左右孩子.具体还是看个例子.</p>

<p><img src="http://yoursite.com/images/radix_tree.png"></p>

<p>这个就是这个radix tree怎么用4位二进制来存2,7,10这三个数的.查询的时候会从根开始根据目的地址的二进制进行搜索,选择左孩子还是右孩子.最后找到结果.</p>

<h2>fast cache对于路由的局限</h2>

<p>由于cache的建立是想避免多余的查表的,也是没有子网掩码的.这里就不能像路由表那样查完整个表然后做最长匹配了.比如172.31.46.0/24,172.31.46.128/25,172.31.46.129/32 如果我们有这3个cache,目的地址是172.31.46.129.我们想一下就找到对应的cache.这就成为cache当中最大的问题. 简单但是不可能的方式就是都用给每个主机做32位的cache.</p>

<p>IOS用了以下规则做cache.</p>

<ol>
<li>直连接口用32位做cache(因为直连,所以目的地址应该在LAN上面,应该是主机,所以用32位)</li>
<li>多条等价路径,用32位做cache.(应该是防止一个目的地址走了不同的路径,最后包会乱序)</li>
<li>超网主类有子网的.都用最长的前缀做cache</li>
<li>主类没有子网,就用主类的.</li>
</ol>


<h2>维护cache</h2>

<p>维护cache主要是指管理cache的老化,删除不再存在的cache.</p>

<h3>去掉不存在的cache</h3>

<p>这个比较麻烦在于路由表是递归查询的.但是cache只有递归查询的结果.如果中途递归查询的其中一步变了.你很难说cache是否还有效.所以简单的维护cache的方式就是直接把cache删除,让他重新做一次查询形成cache.很多情况都会触发cache的删除,比如说这个条目的下一跳变了或者这个路由条目更新了,下一跳的ARP超时了等等.</p>

<h3>cache老化</h3>

<p>这里说的cache的老化其实是为了省一些内存空间.简单的说就是内存空间越小,他就随机去老化一些cache条目.</p>

<h2>Fast switch的负载均衡</h2>

<p>fast switch是不可能像process switch做每个包的负载均衡,原因就是因为路由层面和转发层面其实已经开始分离.</p>

<p><img src="http://yoursite.com/images/fast_switching_balancing.png"></p>

<p>你可以想象到如果多个client给server发连接的话,由于A上面cache的存在,A只会选取AB之间的一条链路使用,另外一条完全空闲出来.如果有多个server的话,单个server肯定选的是一条路.多个之间倒是有一定概率负载均衡下.如果反过来看,server回到client的traffic,还有可能选择那条空闲的,你可以说这样反而负载了一部分,然而这样也造成来回的路由不对称.</p>

<h1>optimum switching</h1>

<p>其实这个很不出名,基本就是对于cache的一点数据结构的优化.Radix tree是个很通用的二叉树,然而IP地址这样的结构固定的玩意,可以用更适合这种结构的数据结构来优化执行效率.这个优化其实就是把一个二叉树变成一个256叉树(mtree).因为IP地址的8个字节就是256bit. 大概就这个样子了.</p>

<p><img src="http://yoursite.com/images/optimum_switch.png"></p>

<h1>CEF</h1>

<p>这本书的时候CEF还是最新的科技.克服了各种缺点,开始就是在各种复杂的大环境中做的测试.12.0之后CEF才是默认的转发模式.还只对某些设备才有.</p>

<p>CEF是直接镜像了路由表和mac表,然后建立了两个表. CEF表和adjacency表.CEF表的实现方式跟optimum switch里面的mtree一模一样.不同的地方在于做cache的时候,里面直接存了下一跳的MAC地址.这里只存了一个指针,指向了adjancency表中的真实转发信息(下一跳,出接口,下一跳的二层地址信息)</p>

<p>通过show adjacency table可以看到出接口直连的相关信息(在以太网中就是IP地址).邻接表中还会有不同的条目类型,例如直连有做过了ARP,有MAC地址,没有MAC地址等等.</p>

<h2>CEF的优势</h2>

<p>CEF其实跟Fast switch还是有类似的之处的.CEF其实最大的优点就是在最开始就提前建好这些内容.省去了cache第一次查询的时间了.当路由表特别大的时候提前构建CEF表和adjacency表还是有很大优势的,这样省去了转发时候构造cache的时间.这两个表在结构上面是独立的.所以不用像原来那样老化这个表.只要独立维护就好.</p>

<h2>CEF的负载均衡</h2>

<p>CEF的负载均衡可以是根据选择根据源目地址或者包级别的负载均衡,而且CEF也克服了之前提到过的单一目的地址负载不均衡的问题.解决方法其实就是在CEF表中找到条目的时候,再用数据包中的源目地址做一定操作,例如XOR之类的,映射到多条路径中的一条.这样当源目地址发生变化的时候,就有一定可能负载均衡了.但是这个负载均衡也只是概率上面的.当条目少,流量不够复杂的时候,是有机会出现散列不够好的情况.</p>

<h2>数据包转发过程</h2>

<p>这里列出来了数据包在设备中的从收到数据包到转发出去整个过程的主要步骤</p>

<ol>
<li>压缩或者解压</li>
<li>加密(这步我不是很明白,收了一个数据包不应该是解密么,我估计是书上的笔误?)</li>
<li>入向ACL</li>
<li>URPF校验(不知道的可以搜下)</li>
<li>入向的限速</li>
<li>如果是个广播包的话,handle一下(比如DHCP里面的 helper address)</li>
<li>TTL减一</li>
<li>高级检测(例如防火墙feature)</li>
<li>outside到inside方向的NAT</li>
<li>处理IP header里面的某些flag</li>
<li>找出接口</li>
<li>找到关于这个包的策略路由</li>
<li>web cache 重定向</li>
<li>inside to outside NAT</li>
<li>加密</li>
<li>出向ACL</li>
<li>再做一个高级防火墙检测</li>
<li>TCP相关检测防止DDOS</li>
</ol>


<p>这个列表我在做troubleshoot的时候自己翻看很多次,还算比较有用.</p>

<h1>总结</h1>

<p>其实从这一章的四个转发模式中你可以看到现在大家天天见到的CEF是一步步如何进化而来的.其中用了什么样的机制克服了当年的什么样的弱点.有兴趣深究的同学可以研究下mtree, radix tree和hash表在选取上面的优劣和他们在做一些基本操作上面的时间复杂度的区别.</p>
]]>
		</content>
	</entry>
	
	<entry>
		
			<title type="html"><![CDATA[Inside Cisco IOS Software Architecture(第一章,系统基础知识)]]></title>
		
		<link href="http://yoursite.com/blog/2016/10/20/inside-cisco-ios-software-architecture-1st/"/>
		<updated>2016-10-20T22:22:09-04:00</updated>
		<id>http://yoursite.com/blog/2016/10/20/inside-cisco-ios-software-architecture-1st</id>
		<content type="html">
			<![CDATA[
				
					<p></p>
				
			]]>
			<![CDATA[<p>由于本书写于1990年代,CEF还是cisco最新的黑科技. 所以其中很多关于操作系统的内容已经不太正确.Cisco的操作系统也从最开始的IOS一种形式到后来的linux做control plane的IOS,IOS-XR,IOS-XE,NX-OS 等等等. 我相信书中提到的很多玩意已经不再使用,或者起码有所变化,但是由于没有更新的版本的书讲新的操作系统的内部,所以还是只能从这本书上了解.从学习的角度来看,从一个比较原始的形态学习也有助于一步步理解更复杂的系统. 所以不要过分纠结书的年代和细节内容.</p>

<h1>操作系统</h1>

<p>首先书中复习了下大学操作系统的基本概念,操作系统本质是用来抽象硬件和做资源分配的.</p>

<p>由于要讲CPU的资源分配,继续复习了大学操作系统中的线程,进程,内核的概念.根据CPU分配资源的情况排队策略不同,分类为FIFO,有优先级的FIFO,可抢占的多任务系统.各种不同排队策略的优劣势大概大家可以想出来,比如实时性无法满足,霸占CPU资源，切换的多了又会出现因为CPU要不停的切换现场带来的资源浪费等等一堆废话,有兴趣的可以复习下大学课本.</p>

<p>除了CPU资源,另外一个很重要的资源就是内存了. 再次帮你复习一下什么是操作系统分配的内存和虚拟内存.然后画风一转,说了一句大废话.IOS系统由于为了保证性能,所以没有完全实现虚拟内存.</p>

<h1>IOS操作系统</h1>

<p>IOS系统历史久远,那个时代大家为了省系统资源基本什么事情都做的出来. 更何况IOS这种系统开始只是为了转发,后来功能是越加越多.所以缺失了一些其他操作系统最基本的功能,比如进程之间连内存的保护都没有.</p>

<p><img src="http://yoursite.com/images/IOS架构.png"></p>

<p>从图上你可以看到系统中的几部分,很有意思的是他不像日常我们现在接触的linux,区分用户态和内核态.看上去更像是一堆功能堆在一起的,我一直以为路由的计算等等功能应该是在用户态实现之后然后通过类似于system call的东西传送给内核态,但是从这个图来看早起的IOS来看应该是根本没有的.</p>

<p>名字就代表了基本功能,还算明显,process应该对应到设备上面应该就是所谓的主控卡所执行的大多数任务.packet buffer就是接口收到所有要转发的包的集合,需要注意的是一些网工(其实就是我)会思考硬件上面的具体某个东西(比如queue)来和这个对应.但是从操作系统的角度来看,永远是CPU和内存在交换数据,所谓的硬件queue之类的东西在这个图里面其实是在device driver层面的东西. 内核就是更多操作系统的基本功能了,CPU和内存资源的调度. 快速转发这一块,下一章再说.</p>

<h2>内存</h2>

<p>内存这一块,IOS基本没有用虚拟内存来扩充内存空间.所以他对内存的使用上面也是直接使用,没有抽象成传统操作的内存页再提供给应用程序.这样目的应该还是为了省出来一些内存资源.</p>

<p>IOS把内存区分不同的区域,称为regions. 这个regions基本就和物理内存是一一对应的关系.其中的DRAM基本就是用来做日常我们接触到的功能,比如用来跑IOS,留给进程存取变量之类的.SRAM是用来做packet buffer做快速转发. (由于大学操作系统也没有学的很好,还是搜了下DRAM和SRAM的区别,简单可以理解为DRAM便宜,SRAM贵,SRAM比较块.)</p>

<p>现在在IOS的路由器上面还是可以用show region命令看到具体IOS的内存分配情况.</p>

<p>这个图就是内存region的划分.</p>

<p><img src="http://yoursite.com/images/IOS内存区域.png"></p>

<p>你真的特别想搞明白具体是干嘛的.我推荐你复习下大学操作系统的课本.简单的可以理解是main里面跑了IOS和IOS运行起来需要的动态的内存空间.PCI和IO就是PCI总线上面和IO设备要用的内存空间.从整个内存的角度大概是这么个情况.</p>

<p>但是一般我们会用到一个show memory的命令,看IOS内存空间使用情况.这个命令就是具体更具体的显示了每一部分内存的使用情况,不过这个命令的输出真的很长.</p>

<h2>进程</h2>

<p>IOS其实是用了一个非抢占的调度CPU的模式.看上去有点蠢,因为网络设备本质是要快速处理来的包,如果不能抢占,包来了你的CPU不能及时转发.这是个非常傻X的事情.我们会在第二章讲包交换架构的时候提到这个事情是怎么解决的. 非抢占的优势还是有的,CPU不用来回保存现场切换来去,效率高了一点,其次对程序员来说编程容易.(这段基本纯翻译,看到最后一句的时候我都惊呆了&hellip;.)</p>

<h3>进程的一生</h3>

<p>其实在IOS里面有一个paser进程,基本负责了大多数创建.paser进程就是在你敲完命令之后分析配置文件,然后根据配置文件做相关的事情.比如你刚敲了router eigrp.他就起了个eigrp进程.你no掉这个eigrp进程的时候,这个parser就会干掉这个进程.</p>

<p><img src="http://yoursite.com/images/IOS进程.png"></p>

<p>图其实已经比较清楚简单的解释了一个进程的一生,其中modification这个状态应该指的是说这个进程创建的时候需要的一些参数,我可以想象到的就是比如你在配置OSPF的时候router id这些参数是这在这个状态被添加到进程当中的,当然这是我的理解,可能完全是错的.</p>

<p>show process这个命令可以看到具体进程的具体信息. IOS当中进程的优先级分4个,Critical,High,Medium,Low.</p>

<ul>
<li>Ciritical: 主要是系统必须要用的基础服务</li>
<li>High: 重要的必须要快速响应的.比如端口收到了一个包</li>
<li>Medium: 大多数系统进程都在这.比如我们用的路由协议</li>
<li>Low: 最不重要的,比如log</li>
</ul>


<p>对于show process更细致的解释,请自行搜索.或者参见inside Cisco IOS software architecture的相关内容.</p>

<h2>CPU资源管理</h2>

<p>在内核当中用来分配CPU资源的就是scheduler, 所有的进程被分配到6个queue当中.6个queue分为三个大类.
- idle queue: 空闲的queue,等待别的事件让他active
- dead queue: 进程结束了,但是内存相关资源还没有被操作系统回收
- ready queue:
  -  Critical</p>

<ul>
<li><p> High</p></li>
<li><p> medium</p></li>
<li><p> low</p></li>
</ul>


<p>分配资源算法简单说起来就是在每执行一个进程之后,都要去看有没有优先级更高的进程应该被执行用来. 如果有,一定保证优先级更好的进程被执行.</p>

<p>通过show process cpu可以看到CPU的使用情况</p>

<p><img src="http://yoursite.com/images/IOS_CPU.png"></p>

<p>其中图中的百分之90和百分之82表示了前五秒CPU的被进程的总消耗和其中具体处理中断的消耗,这里百分之8的差值主要指的是相互进程之间切换导致的损耗.最后剩下的百分之10,就是scheduler本身所消耗掉的CPU资源.</p>

<p>我们之前提到过IOS是通过一个非抢占的方式分配CPU资源,这样就有一个很常见的问题.一个资源如果霸占着CPU不放手,那要怎么办. 在IOS当中有个玩意叫watchdog timer来解决这个问题. 工作原理就是一个进程启动之后,开始对这个进程计时,当过了2秒之后,这个进程还是占着CPU资源.他就把CPU的控制权还给scheduler,如果scheduler还是把CPU给这个程序,而且这个资源继续霸占CPU2秒.这个进程就会被干掉.</p>

<h2>内存资源管理</h2>

<p>这里就是之前提到过的region manager把物理内存划分成不同的region. pool manager就是分配动态是内存资源.系统最开始启动的时候是一块大的内存资源,当不同进程申请释放之后.内存块就会变得多个小块,pool manager就是尽量保证释放后的内存能有尽量大单块资源. show memory和show memory free可以看到具体进程使用内存的情况.</p>

<p>pool manager有一个缺点就是每个管理的block都有一个32byte的overhead,当内存block很大的时候,这点overhead无所谓.当有多个小block的时候,这个overhead就很要命了.这里IOS就有一个chunk manager来解决这个问题. chunk manager其实就是把一个大的block分成N个大小固定的chunk,当进程申请内存的时候,chunk manager就会把几个chunk给这个进程.这个chunk manager整个block才有一个32byte的overhead.所以比pool manager每个block多一个32byte的overhead省了不少内存空间.</p>

<h3>内存分配问题</h3>

<p>之前都提到的都是分配成功的.内存也有分配不成功的时候,主要有两个原因.一个就是内存空间不够了.还有一种可能就是内存空间虽然够,但是最大的block不满足进程的需求. show memory可以看到当前最大的内存block是多少.如果有进程需求比这个block要大,就会导致进程无法创建.</p>

<h1>Packet Buffer 管理</h1>

<p>IOS系统会在内存创建一块区域专门存放要转发的数据包.在IOS系统内部管理packet buffer的是buffer pool manager.</p>

<p>IOS系统当中通过packet buffer manager会创建不同种类的packet buffer pool,其实这个pool也就是一块内存区域.它根据不同的类型可能是静态的,动态的.所有进程都可以访问的,或者是某个进程才能访问的.</p>

<h2>系统Buffer</h2>

<p>IOS操作系统有一个公共的buffer叫系统buffer.这个buffer就是用来处理收到的数据包,也存放设备自己产生的数据包.通过show buffer命令可以看到具体的信息.</p>

<p>系统buffer会根据MTU的不同,又分为不同的队列(具体这样处理是不是比较快,我也不能确定,可能是因为CPU处理固定长度的会有很大的优势,起码直接根据长度直接提取出相应的字节). 具体show出来的内容解释就需要参考书了.</p>

<p>一个简单的例子说明下buffer的工作原理.</p>

<p><img src="http://yoursite.com/images/系统buffer.png"></p>

<p>这是一个初始状态的用来放104byte包大小的buffer.</p>

<p>在前8个包进入buffer的时候只会简单的占用8个,剩下8个.此时请注意,剩下的buffer数量已经等于min buffer的大小.此时,如果还有包进入buffer.系统会尝试保持最小buffer,换句话说,如果这个时候进来了4个包,也就是一共12个包进入buffer.这时buffer会从16涨到20.请注意,这里并非无限增长的.如果继续有包进入buffer,buffer就会停止涨下去.多余的包就会被丢弃.(这里为什么是从16涨到20,书上也并非说的特别清楚,只是举例说show出来buffer不同的状态.根据我的猜测,应该是在一个时间间隔内尝试满足buffer的需求,即使这样,应该会也有个上限.书中完全没有提到,在次我们只能做大概了解)</p>

<p>最终状态的buffer样子:</p>

<p><img src="http://yoursite.com/images/系统buffer最后.png"></p>

<p>其中可以看到此时buffer大小其实是20,永久buffer是16,最后会释放多余的内存空间.20 hits指的就是最后buffer能接受数据包的数量.13 miss这个计数器是当buffer小于min buffer的时候他就会增长.4 created就是动态新建了4个buffer.最后有一个包被丢弃标示为1 failure. 如果觉得此处解释的不清楚.请参考书上例子,有图解.</p>

<h1>设备驱动</h1>

<p>毋庸置疑,所有硬件设备都需要驱动.IOS也不例外,不过值得一提的就是IOS网卡驱动分为两大块.一个是控制部分,比如shut down端口.另外一部分是流量转发.第二章会讲到网卡驱动会和包转发紧密联系在一起.</p>

<p>IOS的驱动通过非常特殊的方式(interface descriptor block)和其他部分沟通(我也不知道一般操作系统通过什么方式). IDB里面存储了接口的IP地址,接口状态,接口相关统计等等等.</p>

<h1>总结</h1>

<p>IOS操作系统其实和其他操作系统区别不大,只有两个是需要很注意的,内存资源有限,对于转发的要求.(我相信现在对于内存的要求变化差距应该很大了,曾经是很缺内存,现在情况并非这样了).</p>

<p>其中IOS比较奇葩的就是把内存平铺起来,所有进程随便用.此处注意,IOS进程相当于别的系统的线程(可能是由于内存可以随便访问所以才这么说?当年也没有多核CPU啊).</p>

<p>IOS不像其他操作系统的一点还有就是根本没有user mode或者说是kernel就跑在user mode下面和其他进程共享内存资源.</p>

<p>下一章,我们讲述IOS系统的转发.</p>
]]>
		</content>
	</entry>
	
</feed>